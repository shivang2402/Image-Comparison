{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the two images\n",
    "# image2 = cv2.imread(\"Bedroom/Holiday Inn - Bedroom Adjustment 2.jpg\")\n",
    "# image1 = cv2.imread(\"Bedroom/Holiday Inn - Bedroom Baseline Pic.jpg\")\n",
    "image2 = cv2.imread(\"Bathroom/Holiday Inn - Bathroom Adjustments 3.jpg\")\n",
    "image1 = cv2.imread(\"Bathroom/Holiday Inn - Bathroom Baseline Pic.jpg\")\n",
    "\n",
    "# Step 2: Convert images to grayscale for optical flow\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 3: Perform optical flow using Lucas-Kanade algorithm\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=5, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "points1 = cv2.goodFeaturesToTrack(gray_image1, mask=None, maxCorners=200, qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "if points1 is not None:\n",
    "    points2, status, _ = cv2.calcOpticalFlowPyrLK(gray_image1, gray_image2, points1, None, **lk_params)\n",
    "\n",
    "    # Filter points and create corresponding keypoints\n",
    "    keypoints1 = [cv2.KeyPoint(x[0][0], x[0][1], 10) for x in points1]\n",
    "    keypoints2 = [cv2.KeyPoint(x[0][0], x[0][1], 10) for x in points2]\n",
    "\n",
    "    # Convert keypoints to numpy arrays\n",
    "    keypoints1 = np.array([kp.pt for kp in keypoints1], dtype=np.float32).reshape(-1, 1, 2)\n",
    "    keypoints2 = np.array([kp.pt for kp in keypoints2], dtype=np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # Filter keypoints based on status\n",
    "    keypoints1 = keypoints1[status == 1]\n",
    "    keypoints2 = keypoints2[status == 1]\n",
    "\n",
    "    # Step 4: Estimate the transformation matrix using RANSAC\n",
    "    M, _ = cv2.findHomography(keypoints1, keypoints2, cv2.RANSAC, 5.0)\n",
    "    aligned_image = cv2.warpPerspective(image1, M, (image2.shape[1], image2.shape[0]))\n",
    "\n",
    "    # Rest of the code remains the same...\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No points found for optical flow.\")\n",
    "\n",
    "# Step 4: Perform histogram matching to adjust for differences in lighting and intensity\n",
    "aligned_image = cv2.cvtColor(aligned_image, cv2.COLOR_BGR2LAB)\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "aligned_image[:, :, 0] = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8)).apply(aligned_image[:, :, 0])\n",
    "image2[:, :, 0] = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8)).apply(image2[:, :, 0])\n",
    "\n",
    "aligned_image = cv2.cvtColor(aligned_image, cv2.COLOR_LAB2BGR)\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "\n",
    "# Step 5: Calculate the absolute difference between the aligned image and image 2\n",
    "diff = cv2.absdiff(aligned_image, image2)\n",
    "gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 6: Denoise the difference image\n",
    "denoised_diff = cv2.fastNlMeansDenoising(gray_diff, h=200, templateWindowSize=7, searchWindowSize=11)\n",
    "\n",
    "# Step 7: Threshold the denoised difference image to identify significant differences\n",
    "threshold = 65  # Adjust this threshold as needed\n",
    "_, thresh = cv2.threshold(denoised_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Step 8: Find contours of the significant differences\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Merge overlapping bounding boxes and filter small boxes\n",
    "contours_merged = []\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    if w * h > 100:  # Adjust this size threshold as needed\n",
    "        contours_merged.append((x, y, x + w, y + h))\n",
    "\n",
    "# Sort the merged contours by their y-coordinate to ensure non-overlapping boxes\n",
    "contours_merged = sorted(contours_merged, key=lambda x: x[1])\n",
    "\n",
    "# Create a separate list to store the non-overlapping contours\n",
    "non_overlapping_contours = []\n",
    "\n",
    "# Iterate over the contours_merged list\n",
    "for x1, y1, x2, y2 in contours_merged:\n",
    "    overlapping = False\n",
    "    for x3, y3, x4, y4 in non_overlapping_contours:\n",
    "        # Check if boxes overlap\n",
    "        if x3 < x2 and x4 > x1 and y3 < y2 and y4 > y1:\n",
    "            overlapping = False\n",
    "            break\n",
    "    # Add the non-overlapping box to the list\n",
    "    if not overlapping:\n",
    "        non_overlapping_contours.append((x1, y1, x2, y2))\n",
    "\n",
    "# Draw non-overlapping bounding boxes on image 2\n",
    "highlighted_image = image2.copy()\n",
    "for x1, y1, x2, y2 in non_overlapping_contours:\n",
    "    cv2.rectangle(highlighted_image, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "# Step 9: Display the side-by-side comparison of the images\n",
    "combined_image = np.concatenate((image1, highlighted_image), axis=1)\n",
    "cv2.imshow(\"Image Comparison\", combined_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
