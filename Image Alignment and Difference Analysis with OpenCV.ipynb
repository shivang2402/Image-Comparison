{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the two images\n",
    "image2 = cv2.imread(\"Bedroom/Holiday Inn - Bedroom Adjustment 2.jpg\")\n",
    "image1 = cv2.imread(\"Bedroom/Holiday Inn - Bedroom Baseline Pic.jpg\")\n",
    "\n",
    "# Step 2: Perform feature detection and matching using SIFT\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=1)\n",
    "search_params = dict(checks=250)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 150 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract the keypoints from the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate the transformation matrix using RANSAC\n",
    "M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "aligned_image = cv2.warpPerspective(image1, M, (image2.shape[1], image2.shape[0]))\n",
    "\n",
    "# Step 4: Perform histogram matching to adjust for differences in lighting and intensity\n",
    "aligned_image = cv2.cvtColor(aligned_image, cv2.COLOR_BGR2LAB)\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "aligned_image[:, :, 0] = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8)).apply(aligned_image[:, :, 0])\n",
    "image2[:, :, 0] = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8)).apply(image2[:, :, 0])\n",
    "\n",
    "aligned_image = cv2.cvtColor(aligned_image, cv2.COLOR_LAB2BGR)\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Step 5: Calculate the absolute difference between the aligned image and image 2\n",
    "diff = cv2.absdiff(aligned_image, image2)\n",
    "gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 6: Denoise the difference image\n",
    "denoised_diff = cv2.fastNlMeansDenoising(gray_diff, h=200, templateWindowSize=7, searchWindowSize=11)\n",
    "\n",
    "# Step 7: Threshold the denoised difference image to identify significant differences\n",
    "threshold = 65  # Adjust this threshold as needed\n",
    "_, thresh = cv2.threshold(denoised_diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Step 8: Find contours of the significant differences\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Merge overlapping bounding boxes and filter small boxes\n",
    "contours_merged = []\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    if w * h > 100:  # Adjust this size threshold as needed\n",
    "        contours_merged.append((x, y, x + w, y + h))\n",
    "\n",
    "# Sort the merged contours by their y-coordinate to ensure non-overlapping boxes\n",
    "contours_merged = sorted(contours_merged, key=lambda x: x[1])\n",
    "\n",
    "# Create a separate list to store the non-overlapping contours\n",
    "non_overlapping_contours = []\n",
    "\n",
    "# Iterate over the contours_merged list\n",
    "for x1, y1, x2, y2 in contours_merged:\n",
    "    overlapping = False\n",
    "    for x3, y3, x4, y4 in non_overlapping_contours:\n",
    "        # Check if boxes overlap\n",
    "        if x3 < x2 and x4 > x1 and y3 < y2 and y4 > y1:\n",
    "            overlapping = False\n",
    "            break\n",
    "    # Add the non-overlapping box to the list\n",
    "    if not overlapping:\n",
    "        non_overlapping_contours.append((x1, y1, x2, y2))\n",
    "\n",
    "# Draw non-overlapping bounding boxes on image 2\n",
    "highlighted_image = image2.copy()\n",
    "for x1, y1, x2, y2 in non_overlapping_contours:\n",
    "    cv2.rectangle(highlighted_image, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "# Step 9: Display the side-by-side comparison of the images\n",
    "combined_image = np.concatenate((image1, highlighted_image), axis=1)\n",
    "cv2.imshow(\"Image Comparison\", combined_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
